prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
set.seed(123)
d(123)
set.seed(123)
n <- 100
X <- cbind(1, rnorm(n))  # Assuming X includes a column of 1's for the intercept
beta <- c(-1, 2)         # Coefficients (intercept = -1, slope = 2)
y <- ifelse((1 / (1 + exp(-X %*% beta))) > 0.5, 1, 0)  # Generate binary labels
# Call your confusion_matrix function
result <- confusion_matrix(beta, X, y, cut_off = 0.5)
result
test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
test_resp <- 1
test_pred <- 2:4
log_betas(Data = test_data, Y = test_resp, X = test_pred)
library(MASS)
#' @author Charles Benfer
#' @import stats
#' @import MASS
#' @export
#' @examples
#'   #data obtained from https://stats.oarc.ucla.edu/r/dae/logit-regression/
#'   test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
#'   test_resp <- 1
#'   test_pred <- 2:4
#'   log_betas(Data = test_data, Y = test_resp, X = test_pred)
log_betas <- function(Data, Y, X){
Data <- cbind(as.matrix(Data[,Y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
if(length(X)>1){
X <- X+1
}
else{X = 3}
start_betas <- ginv(t(as.matrix(Data[,c(2,X)]))%*%as.matrix(Data[,c(2,X)]))%*%t(as.matrix(Data[,c(2,X)]))%*%as.matrix(Data[,Y])
optim(start_betas, log_loss, Obs = Data, Resp=Y, Preds=X)
}
log_betas(Data = test_data, Y = test_resp, X = test_pred)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-X %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(y == 1 & predicted_labels == 1)
FP <- sum(y == 0 & predicted_labels == 1)
FN <- sum(y == 1 & predicted_labels == 0)
TN <- sum(y == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
test_resp <- 1
test_pred <- 2:4
result <- confusion_matrix(beta, X, y, cut_off = 0.5)
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-t(X) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(y == 1 & predicted_labels == 1)
FP <- sum(y == 0 & predicted_labels == 1)
FN <- sum(y == 1 & predicted_labels == 0)
TN <- sum(y == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-t(Data[,X]) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(y == 1 & predicted_labels == 1)
FP <- sum(y == 0 & predicted_labels == 1)
FN <- sum(y == 1 & predicted_labels == 0)
TN <- sum(y == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
Data <- cbind(as.matrix(Data[,y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-t(Data[,X]) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(y == 1 & predicted_labels == 1)
FP <- sum(y == 0 & predicted_labels == 1)
FN <- sum(y == 1 & predicted_labels == 0)
TN <- sum(y == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
Data <- cbind(as.matrix(Data[,y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-t(Data[,c(2,X)]) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(y == 1 & predicted_labels == 1)
FP <- sum(y == 0 & predicted_labels == 1)
FN <- sum(y == 1 & predicted_labels == 0)
TN <- sum(y == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
Data <- test_data
X<- 2:4
y<-1
Data <- cbind(as.matrix(Data[,y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
View(Data)
beta <- log_betas(Data = Data, X = X, Y = y)$par
View(beta)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
Data <- cbind(as.matrix(Data[,y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-(Data[,c(2,X)]) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(y == 1 & predicted_labels == 1)
FP <- sum(y == 0 & predicted_labels == 1)
FN <- sum(y == 1 & predicted_labels == 0)
TN <- sum(y == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
print(result)
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
Data <- cbind(as.matrix(Data[,y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-(Data[,c(2,X)]) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(Data[,y] == 1 & predicted_labels == 1)
FP <- sum(Data[,y] == 0 & predicted_labels == 1)
FN <- sum(Data[,y] == 1 & predicted_labels == 0)
TN <- sum(Data[,y] == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
result <- confusion_matrix(Data = test_data, X=test_pred, y=test_resp, cut_off = 0.5)
print(result)
#' a \code{vector} of observations and a \code{matrix} of coefficients
#' @param Obs A \code{matrix} containing observations we want to
#' create a logistic model for.
#' @param Resp An \code{integer} that tells which column of the observation matrix you
#' want to be the response variable.
#' @param Preds A \code{vector} of integers corresponding to the columns for the desired predictor variables
#' @return A \code{numeric} giving value of loss at \code{beta}
#' @author Charles Benfer
#' @importFrom stats
#' @export
log_loss <- function(Beta, Obs, Resp, Preds){
Obs <- cbind(as.matrix(Obs[,Resp]),rep(1, nrow(Obs)), as.matrix(Obs[,Preds]))
p <- rep(NA, nrow(Obs))
for(i in 1:nrow(Obs)){
p[i] <- 1/(1+exp(-t(Obs[i,c(2,Preds)])%*%Beta))
}
(t(log(p))%*%(-Obs[,Resp]) - t(log(1-p))%*%(1-Obs[,Resp]))
}
#' @author Charles Benfer
#' @import stats
#' @import MASS
#' @export
#' @examples
#'   #data obtained from https://stats.oarc.ucla.edu/r/dae/logit-regression/
#'   test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
#'   test_resp <- 1
#'   test_pred <- 2:4
#'   log_betas(Data = test_data, Y = test_resp, X = test_pred)
log_betas <- function(Data, Y, X){
Data <- cbind(as.matrix(Data[,Y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
if(length(X)>1){
X <- X+1
}
else{X = 3}
start_betas <- ginv(t(as.matrix(Data[,c(2,X)]))%*%as.matrix(Data[,c(2,X)]))%*%t(as.matrix(Data[,c(2,X)]))%*%as.matrix(Data[,Y])
optim(start_betas, log_loss, Obs = Data, Resp=Y, Preds=X)
}
#' @param Data A \code{matrix} containing the data we want to plot
#' @param Resp A \code{integer} representing the response variable column.
#' @param pred A \code{integer} representing the predictor variable column.
#' @return A plot showing the logistic regression line.
#' @author Ernest Asante
#' @import stats
#' @export
#' @examples
#'   test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
#'   logistic_plot(test_data, Resp = 1, Pred= 3)
logistic_plot <- function(Data, Pred, Resp){
coeffs <- log_betas(Data = Data, Y = Resp, X = Pred)$par
curve <- function(x){(exp(coeffs[1] + coeffs[2]*x))/(1 + exp(coeffs[1] + coeffs[2]*x))}
plot(Data[,Resp]~Data[,Pred], ylab = 'Response', xlab = colnames(Data)[Pred])
plot(curve, add = T)
}
confusion_matrix <- function(Data, X, y, cut_off = 0.5) {
#calculate the beta values
Data <- cbind(as.matrix(Data[,y]),rep(1, nrow(Data)), as.matrix(Data[,X]))
beta <- log_betas(Data = Data, X = X, Y = y)$par
# first of all Calculate the predicted probabilities
predicted_probs <- (1 / (1 + exp(-(Data[,c(2,X)]) %*% beta)))
# Convert continuous probabilities to binary classification labels
predicted_labels <- ifelse(predicted_probs > cut_off, 1, 0)
# Compare our predictions and our responses y
TP <- sum(Data[,y] == 1 & predicted_labels == 1)
FP <- sum(Data[,y] == 0 & predicted_labels == 1)
FN <- sum(Data[,y] == 1 & predicted_labels == 0)
TN <- sum(Data[,y] == 0 & predicted_labels == 0)
total_positive <- TP + FN
total_negative <- FP + TN
# Create the list for result
confuse_matrix <- matrix(c(TN, FP, FN, TP), nrow = 2)
rownames(confuse_matrix) <-  c("Actual Positive", "Actual Negative")
colnames(confuse_matrix) <-  c("Predicted Positive", "Predicted Negative")
prevalence <- total_positive / (total_positive + total_negative)
accuracy <- (TP + TN) / (total_positive + total_negative)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
false_discovery_rate <- FP / (FP + TP)
diagnostic_odds_ratio <- (TP * TN) / (FP * FN)
# Create the list and return it
final_confusion_matrix <- list(
confuse_matrix = confuse_matrix,
prevalence = prevalence,
accuracy = accuracy,
sensitivity = sensitivity,
specificity = specificity,
false_discovery_rate = false_discovery_rate,
diagnostic_odds_ratio = diagnostic_odds_ratio
)
return(final_confusion_matrix)
}
#' @author Group 11
#' @importFrom
#' @export
#' @examples
#' #data obtained from https://stats.oarc.ucla.edu/r/dae/logit-regression/
#' test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
#'   test_resp <- 1
#'   test_pred <- 2:4
#' result <- confusion_matrix(Data = test_data, X= test_pred, y = test_resp, cut_off = 0.5)
#' plot_confusion_matrix(y,X,beta, value_name = "accuracy")
plot_confusion_matrix <- function(y, X, beta, value_name = "prevalence"){
cut_off_value <- seq(0.1,0.9, by = 0.1)
confusion_matrix_value <- matrix(NA, nrow = 6, ncol = 9)
rownames(confusion_matrix_value) <- c("prevalence","accuracy", "sensitivity", "specificity", "false_discovery_rate","diagnostic_odds_ratio")
colnames(confusion_matrix_value) <- seq(0.1, 0.9, by=0.1)
for(i in 1:9){
result <- confusion_matrix(beta, X, y, cut_off = cut_off_value[i])
confusion_matrix_value[1,i] <- result$prevalence
confusion_matrix_value[2,i] <- result$accuracy
confusion_matrix_value[3,i] <- result$sensitivity
confusion_matrix_value[4,i] <- result$specificity
confusion_matrix_value[5,i] <- result$false_discovery_rate
confusion_matrix_value[6,i] <- result$diagnostic_odds_ratio
}
if(value_name %in% rownames(confusion_matrix_value)){
selected_data <- confusion_matrix_value[value_name,]
main_title <- paste(value_name, "vs", "cut_off_value" )
plot(selected_data,cut_off_value, type = "p", col = "blue"
, xlab="cut-off value"
, ylab= value_name
, main = main_title)
grid()
} else {
cat(value_name, "do not exist.\n")
}
}
test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
test_resp <- 1
test_pred <- 2:4
result <- confusion_matrix(Data = test_data, X= test_pred, y = test_resp, cut_off = 0.5)
plot_confusion_matrix(y,X,beta, value_name = "accuracy")
#' @author Group 11
#' @importFrom
#' @export
#' @examples
#' #data obtained from https://stats.oarc.ucla.edu/r/dae/logit-regression/
#' test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
#'   test_resp <- 1
#'   test_pred <- 2:4
#' result <- confusion_matrix(Data = test_data, X= test_pred, y = test_resp, cut_off = 0.5)
#' plot_confusion_matrix(y = test_resp,X = test_pred , value_name = "accuracy")
plot_confusion_matrix <- function(Data, y, X, value_name = "prevalence"){
cut_off_value <- seq(0.1,0.9, by = 0.1)
confusion_matrix_value <- matrix(NA, nrow = 6, ncol = 9)
rownames(confusion_matrix_value) <- c("prevalence","accuracy", "sensitivity", "specificity", "false_discovery_rate","diagnostic_odds_ratio")
colnames(confusion_matrix_value) <- seq(0.1, 0.9, by=0.1)
for(i in 1:9){
result <- confusion_matrix(Data = Data, X = X, y = y, cut_off = cut_off_value[i])
confusion_matrix_value[1,i] <- result$prevalence
confusion_matrix_value[2,i] <- result$accuracy
confusion_matrix_value[3,i] <- result$sensitivity
confusion_matrix_value[4,i] <- result$specificity
confusion_matrix_value[5,i] <- result$false_discovery_rate
confusion_matrix_value[6,i] <- result$diagnostic_odds_ratio
}
if(value_name %in% rownames(confusion_matrix_value)){
selected_data <- confusion_matrix_value[value_name,]
main_title <- paste(value_name, "vs", "cut_off_value" )
plot(selected_data,cut_off_value, type = "p", col = "blue"
, xlab="cut-off value"
, ylab= value_name
, main = main_title)
grid()
} else {
cat(value_name, "do not exist.\n")
}
}
test_data <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
test_resp <- 1
test_pred <- 2:4
result <- confusion_matrix(Data = test_data, X= test_pred, y = test_resp, cut_off = 0.5)
plot_confusion_matrix(y = test_resp,X = test_pred , value_name = "accuracy")
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = "accuracy")
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = "prevalence")
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = "sensativity")
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = "sensitivity")
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = c("prevalence","accuracy", "sensitivity", "specificity", "false_discovery_rate","diagnostic_odds_ratio"))
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = 'false_discovery_rate')
library(devtools)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
library(log.reg.11)
?confusion_matrix
?log_betas
?log_loss
?logistic_plot
?plot_confusion_matrix
plot_confusion_matrix(Data = test_data, y = test_resp,X = test_pred , value_name = 'false_discovery_rate')
library(pkgdown)
pkgdown::build_site()
